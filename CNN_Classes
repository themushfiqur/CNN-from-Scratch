import numpy as np
from Activation_Functions import ReLU, dReLU, Sigmoid, dSigmoid, Softmax, dSoftmax

class ConvLayer:
    def __init__(self, filter_number, filter_shape, stride, padding, activation_fn):
        self.filter_number = filter_number
        self.filter_shape = filter_shape
        self.stride = stride
        self.padding = padding
        self.activation_fn = activation_fn
        self.f = None
        self.W = None
        self.b = None

    def initialize_filter(self, f_c, filter_number, filter_shape):
        f_h = filter_shape[0]
        f_w = filter_shape[1]
        np.random.seed(42)
        self.f = np.random.randn(filter_number, f_c, f_h, f_w)
        self.b = np.zeros((filter_number, 1, 1))

    @staticmethod
    def zeropadding(X, n_h_prev1, n_w_prev1, f_h, f_w, s_h, s_w, padding):
        if padding == 'same':
            p_h = ((n_h_prev1 - 1) * s_h - n_h_prev1 + f_h) / 2
            if p_h % 1 == 0:
                p_h_u = p_h
                p_h_l = p_h
            else:
                p_h_u = int(np.ceil(p_h))
                p_h_l = int(np.floor(p_h))

            p_w = ((n_w_prev1 - 1) * s_w - n_w_prev1 + f_w) / 2
            if p_w % 1 == 0:
                p_w_l = p_w
                p_w_r = p_w
            else:
                p_w_l = int(np.ceil(p_w))
                p_w_r = int(np.floor(p_w))

            x = np.pad(X, ((0, 0), (0, 0), (p_h_u, p_h_l), (p_w_l, p_w_r)))

        elif padding == 'valid':
            x = np.pad(X, ((0, 0), (0, 0), (0, 0), (0, 0)))

        return x

    @staticmethod
    def filter2matrix(X, f, s_h, s_w):
        x_mat = (X.reshape(X.shape[0], -1)).T
        n_samp, n_c_prev, n_h_prev, n_w_prev = X.shape
        a, b, c, d = f.shape

        n_h = int(np.floor(((n_h_prev - c) / s_h) + 1))
        n_w = int(np.floor(((n_w_prev - d) / s_w) + 1))

        unit_seg_num = n_h * n_w
        slice_px_num = n_h_prev * n_w_prev

        W = np.zeros((n_samp, a*unit_seg_num*x_mat.shape[0]))

        for ii in range(n_samp):
            for i in range(a):
                for j in range(unit_seg_num):
                    for k in range(b):
                        for l in range(c):
                            for m in range(d):
                                W[ii, (i * unit_seg_num + j) * x_mat.shape[0] + k * slice_px_num +
                                  int(np.floor(j / n_w)) * s_h * n_w_prev + \
                                  (j - int(np.floor(j / n_w)) * n_w) * s_w + l * n_w_prev + m] = f[i, k, l, m]

        return W, a, n_h, n_w

    @staticmethod
    def conv2matmul(X, W, a, n_h, n_w):
        x_mat = (X.reshape(X.shape[0], -1)).T
        unit_seg_num = n_h * n_w
        n_samp = W.shape[0]
        Z = np.zeros((n_samp, a*unit_seg_num))
        for ii in range(n_samp):
            for i in range(a):
                for j in range(unit_seg_num):
                    Z[ii, i * unit_seg_num + j] = W[ii, (i * unit_seg_num + j) * x_mat.shape[0] : (i * unit_seg_num + j + 1) * x_mat.shape[0]] @ x_mat[:, ii]

        return Z

    @staticmethod
    def conv2matmul2(X, f, s_h, s_w):
        x_mat = X.reshape(X.shape[0], X.shape[1], -1)
        n_samp, n_c_prev, n_h_prev, n_w_prev = X.shape
        a, b, c, d = f.shape

        n_h = int(np.floor(((n_h_prev - c) / s_h) + 1))
        n_w = int(np.floor(((n_w_prev - d) / s_w) + 1))

        unit_seg_num = n_h * n_w
        Z = np.zeros((n_samp, a * unit_seg_num))

        for ii in range(n_samp):
            for i in range(a):
                for j in range(unit_seg_num):
                    summ = 0
                    for k in range(b):
                        W = np.zeros((1, (n_h_prev * n_w_prev)))
                        for l in range(c):
                            for m in range(d):
                                W[0, int(np.floor(j / n_w)) * s_h * n_w_prev + \
                                  (j - int(np.floor(j / n_w)) * n_w) * s_w + l * n_w_prev + m] = f[i, k, l, m]
                        summ += np.sum((W * x_mat[ii, k, :]), axis=1)
                    Z[ii, i * unit_seg_num + j] = summ

        return Z, a, n_h, n_w

    def activation_function(self, Z, derivative=False):
        if self.activation_fn == 'ReLU':
            if derivative:
                return dReLU(Z)
            return ReLU(Z)
        if self.activation_fn == 'Sigmoid':
            if derivative:
                return dSigmoid(Z)
            return Sigmoid(Z)
        if self.activation_fn == 'Softmax':
            if derivative:
                return dSoftmax(Z)
            return Softmax(Z)


    def forward(self, X):
        self.input = np.array(X, copy=True)
        if self.f is None:
            self.initialize_filter(self.input.shape[1], self.filter_number, self.filter_shape)

        self.paddedinput = self.zeropadding(self.input, self.input.shape[2], self.input.shape[3], self.filter_shape[0],
                              self.filter_shape[1], self.stride[0], self.stride[1], self.padding)

        self.unshapedZ, a, n_h, n_w = self.conv2matmul2(self.paddedinput, self.f, self.stride[0], self.stride[1])
        self.Z = (self.unshapedZ.reshape(self.input.shape[0], a, n_h, n_w)) + self.b

        if self.activation_fn is not None:
            self.A = self.activation_function(self.Z)
            return self.A
        return self.Z

    def backward(self, dA):
        dA = dA.reshape(dA.shape[0], -1)
        _, _, n_h_prev, n_w_prev = self.input.shape
        n_samp, a, n_h, n_w = self.Z.shape
        Z = self.Z.reshape(self.Z.shape[0], -1)
        dZ = self.activation_function(Z, derivative=True)
        x_mat = (self.input.reshape(self.input.shape[0], self.input.shape[1], -1))

        n_f, f_c, f_h, f_w = self.f.shape
        df = np.zeros((n_f, f_c, (f_h * f_w)))
        unit_filter_element = f_h * f_w
        unit_seg = n_h * n_w
        for ii in range(n_samp):
            for i in range(n_f):
                for k in range(f_c):
                    filter_element_mat = np.zeros((1, unit_filter_element))
                    for j in range(unit_seg):
                        filter_element_mat2 = np.zeros((1, unit_filter_element))
                        for l in  range(f_h):
                            for m in range(f_w):
                                filter_element_mat2[0, l * f_w + m] = x_mat[ii, k, int(np.floor(j / n_w)) * \
                                    self.stride[0] * n_w_prev + (j - int(np.floor(j / n_w)) * n_w) * \
                                    self.stride[1] + l * n_w_prev + m]

                        filter_element_mat2 = dA[ii, i * unit_seg + j] * dZ[ii, i * unit_seg + j] * \
                                              filter_element_mat2
                        filter_element_mat += filter_element_mat2
                    df[i, k, :] += filter_element_mat.reshape(-1)

        df = (1 / n_samp) * df
        df = df.reshape(self.f.shape)
        self.df = df

        db = np.zeros((n_f, 1))
        pseudo_db = dA * dZ
        pseudo_db = (1 / n_samp) * np.sum(pseudo_db, axis=0, keepdims=True)

        for i in range(n_f):
            db[i, 0] = np.sum(pseudo_db[0, i * unit_seg: (i + 1) * unit_seg])
        db = db.reshape(self.b.shape)
        self.db = db

        pseudo_dx = np.zeros(x_mat.shape)
        for ii in range(n_samp):
            for k in range(f_c):
                summm = np.zeros((1, (n_h_prev * n_w_prev)))
                for i in range(n_f):
                    for j in range(unit_seg):
                        W = np.zeros((1, (n_h_prev * n_w_prev)))
                        for l in range(f_h):
                            for m in range(f_w):
                                W[0, int(np.floor(j / n_w)) * self.stride[0] * n_w_prev + \
                                  (j - int(np.floor(j / n_w)) * n_w) * self.stride[1] + l * n_w_prev + m] = self.f[i, k, l, m]
                        summm += W * dA[ii, i * unit_seg + j] * dZ[ii, i * unit_seg + j]
                pseudo_dx[ii, k, :] = summm

        pseudo_dx = pseudo_dx.reshape(self.input.shape)
        self.dx = pseudo_dx
        return self.df, self.db, self.dx


class PoolingLayer:
    def __init__(self, filter_shape, stride, pooling):
        self.filter_shape = filter_shape
        self.stride = stride
        self.pooling = pooling

    @staticmethod
    def filter2matrix(x, f, stride, pooling):
        x_new = x.reshape(x.shape[0], x.shape[1], -1)
        n_samp, n_c_prev, n_h_prev, n_w_prev = x.shape
        n_h = int(np.floor(((n_h_prev - f[0]) / stride[0]) + 1))
        n_w = int(np.floor(((n_w_prev - f[1]) / stride[1]) + 1))
        seg_num = n_h * n_w

        W = np.zeros((n_samp, n_h * n_w * n_c_prev * (n_h_prev * n_w_prev)))

        if pooling == 'max':
            for m_idx in range(n_samp):
                for i in range(n_c_prev):
                    for j in range(seg_num):
                        main_idx_storing_mat = np.zeros((1, f[0] * f[1]), dtype=int)
                        sub_idx_storing_mat = np.zeros((1, f[0] * f[1]), dtype=int)
                        val_storing_mat = np.zeros((1, f[0] * f[1]))
                        for ii in range(f[0] * f[1]):
                            i_1 = int(np.floor(ii / f[1]))
                            i_2 = ii - i_1 * f[1]
                            main_idx_storing_mat[0, ii] = (i * seg_num + j) * (n_h_prev * n_w_prev) + \
                             int(np.floor(j / n_w)) * stride[0] * n_w_prev + \
                             (j - int(np.floor(j / n_w)) * n_w) * stride[1] + i_1 * n_w_prev + i_2
                            sub_idx_storing_mat[0, ii] = int(np.floor(j / n_w)) * stride[0] * n_w_prev + \
                             (j - int(np.floor(j / n_w)) * n_w) * stride[1] + i_1 * n_w_prev + i_2
                            val_storing_mat[0, ii] = x_new[m_idx, i, sub_idx_storing_mat[0, ii]]

                        mv_inf = np.where(val_storing_mat == np.amax(val_storing_mat, axis=1))
                        mv = mv_inf[1][0]
                        mvi = main_idx_storing_mat[0, mv]
                        W[m_idx, mvi] = 1

        elif pooling == 'average':
            for m_idx in range(n_samp):
                for i in range(n_c_prev):
                    for j in range(seg_num):
                        for ii in range(f[0] * f[1]):
                            i_1 = int(np.floor(ii / f[1]))
                            i_2 = ii - i_1 * f[1]
                            W[m_idx, (i * seg_num + j) * (n_h_prev * n_w_prev) + \
                              int(np.floor(j / n_w)) * stride[0] * n_w_prev + \
                              (j - int(np.floor(j / n_w)) * n_w) * stride[1] + i_1 * n_w_prev + i_2] = 1 / (f[0] * f[1])

        return W, n_c_prev, n_h, n_w, x_new

    @staticmethod
    def pool2matmul(W, x, n_c_prev, n_h, n_w, x_new):
        _, _, n_h_prev, n_w_prev = x.shape
        unit_seg_num = n_h * n_w
        n_samp = W.shape[0]
        Z = np.zeros((W.shape[0], unit_seg_num * n_c_prev))
        for ii in range(n_samp):
            for i in range(n_c_prev):
                for j in range(unit_seg_num):
                    Z[ii, i * unit_seg_num + j] = W[ii, (i * unit_seg_num + j) * (n_h_prev * n_w_prev):
                                                        (i * unit_seg_num + j + 1) * (n_h_prev * n_w_prev)] @ x_new[ii, i, :]

        return Z

    def forward(self, X):
        self.input = np.array(X, copy=True)
        self.W, n_c_prev, n_h, n_w, x_new = self.filter2matrix(self.input, self.filter_shape, self.stride, self.pooling)
        self.unshapedZ = self.pool2matmul(self.W, self.input, n_c_prev, n_h, n_w, x_new)
        self.Z = self.unshapedZ.reshape(self.W.shape[0], n_c_prev, n_h, n_w)
        self.A = self.Z
        return self.A

    def backward(self, dA):
        dA = dA.reshape(dA.shape[0], -1)
        n_samp, n_c_prev, n_h, n_w = self.Z.shape
        unit_seg = n_h * n_w

        pseudo_dx = np.zeros((n_samp, (self.input.shape[1] * self.input.shape[2] * self.input.shape[3])))
        for ii in range(n_samp):
            pseudo_dx2 = np.zeros((1, (self.input.shape[1] * self.input.shape[2] * self.input.shape[3])))
            for i in range(n_c_prev):
                for j in range(unit_seg):
                    pseudo_dx2[0, i * (self.input.shape[2] * self.input.shape[3])] += self.W[ii,
                                  (i * unit_seg + j) * (self.input.shape[2] * self.input.shape[3]): \
                                  (i * unit_seg + j + 1) * (self.input.shape[2] * self.input.shape[3])] * \
                                  dA[ii, i * unit_seg + j]

            pseudo_dx[ii, :] = pseudo_dx2

        pseudo_dx = pseudo_dx.reshape(self.input.shape)
        self.dx = pseudo_dx
        return self.dx


class FlattenLayer:
    def __init__(self):
        pass

    def forward(self, X):
        self.input = np.array(X, copy=True)
        self.A = self.input.reshape(self.input.shape[0], -1)
        return self.A

    def backward(self, dZ, W):
        self.dA = dZ @ W.T
        self.dx = self.dA.reshape(self.input.shape)
        return self.dA, self.dx


class DenseLayer:
    def __init__(self, hidden_units: int, activation: str=None):
        self.hidden_units = hidden_units
        self.activation = activation
        self.W = None
        self.b = None

    def initialize_parameter(self, n_in, hidden_units):
        np.random.seed(42)
        self.W = np.random.randn(n_in, hidden_units) * np.sqrt(2/n_in)
        self.b = np.zeros((1, hidden_units))

    def forward(self, X):
        self.input = np.array(X, copy=True)
        if self.W is None:
            self.initialize_parameter(self.input.shape[-1], self.hidden_units)
        self.Z = X @ self.W + self.b
        if self.activation is not None:
            self.A = self.activation_function(self.Z)
            return self.A
        return self.Z

    def activation_function(self, Z, derivative=False):
        if self.activation == 'ReLU':
            if derivative:
                return dReLU(Z)
            return ReLU(Z)
        if self.activation == 'Sigmoid':
            if derivative:
                return dSigmoid(Z)
            return Sigmoid(Z)
        if self.activation == 'Softmax':
            if derivative:
                return dSoftmax(Z)
            return Softmax(Z)

    def backward(self, dZ_s, W_s, m):
        self.dZ = dZ_s @ W_s.T * self.activation_function(self.Z, derivative=True)
        self.dW = (1 / m) * self.input.T @ self.dZ
        self.db = (1 / m) * np.sum(self.dZ, axis=0, keepdims=True)
        return self.dZ, self.dW, self.db


class TransConvLayer:
    def __init__(self, filter_number, filter_shape, stride, padding, activation_fn):
        self.filter_number = filter_number
        self.filter_shape = filter_shape
        self.stride = stride
        self.padding = padding
        self.activation_fn = activation_fn
        self.f = None
        self.b = None

    def initialize_filter(self, f_c, filter_number, filter_shape):
        f_h = filter_shape[0]
        f_w = filter_shape[1]
        np.random.seed(42)
        self.f = np.random.randn(filter_number, f_c, f_h, f_w) * np.sqrt(2/(f_h * f_w))
        self.b = np.zeros((filter_number, 1, 1))

    @staticmethod
    def transconv2matmul(X, f, stride, padding):
        n_h = (X.shape[2] - 1) * stride[0] + f.shape[2]
        n_w = (X.shape[3] - 1) * stride[1] + f.shape[3]

        Z = np.zeros((X.shape[0], f.shape[0], n_h, n_w))

        for ii in range(X.shape[0]):
            for ij in range(f.shape[0]):
                for ik in range(X.shape[1]):
                    for i in range(X.shape[2]):
                        for j in range(X.shape[3]):
                            W = np.zeros((1, (n_h * n_w)))
                            for k in range(f.shape[2]):
                                for l in range(f.shape[3]):
                                    W[0, i * stride[0] * n_w + j * stride[1] + k * n_w + l] = f[ij, ik, k, l]
                            f_mul = X[ii, ik, i, j] * W
                            f_mul = f_mul.reshape(n_h, n_w)
                            Z[ii, ij, :, :] += f_mul

        if padding == 'valid':
            Z = Z
            return Z
        elif padding == 'same':
            p_h = (X.shape[2] * (stride[0] - 1) - stride[0] + f.shape[2]) / 2
            if p_h % 1 == 0:
                p_h_u = int(p_h)
                p_h_l = int(p_h)
            else:
                p_h_u = int(np.ceil(p_h))
                p_h_l = int(np.floor(p_h))

            p_w = (X.shape[3] * (stride[1] - 1) - stride[1] + f.shape[3]) / 2
            if p_w % 1 == 0:
                p_w_l = int(p_w)
                p_w_r = int(p_w)
            else:
                p_w_l = int(np.ceil(p_w))
                p_w_r = int(np.floor(p_w))

            pseudo_Z = np.zeros(Z.shape)
            row_start = p_h_u
            row_end = p_h_u + (n_h - p_h_u - p_h_l)
            col_start = p_w_l
            col_end = p_w_l + (n_w - p_w_l - p_w_r)
            pseudo_Z[:, :, row_start:row_end, col_start:col_end] = Z[:, :, row_start:row_end, col_start:col_end]
            Z = pseudo_Z[:, :, row_start:row_end, col_start:col_end]
            return Z, pseudo_Z, row_start, row_end, col_start, col_end

    def activation_function(self, Z, derivative=False):
        if self.activation_fn == 'ReLU':
            if derivative:
                return dReLU(Z)
            return ReLU(Z)
        if self.activation_fn == 'Sigmoid':
            if derivative:
                return dSigmoid(Z)
            return Sigmoid(Z)
        if self.activation_fn == 'Softmax':
            if derivative:
                return dSoftmax(Z)
            return Softmax(Z)

    def forward(self, X):
        self.input = np.array(X, copy=True)
        if self.f is None:
            self.initialize_filter(self.input.shape[1], self.filter_number, self.filter_shape)

        if self.padding == 'valid':
            self.Z = self.transconv2matmul(self.input, self.f, self.stride, self.padding)
        elif self.padding == 'same':
            self.Z, self.pseudoZ, self.row_start, self.row_end, self.col_start, self.col_end = self.transconv2matmul(self.input, self.f, self.stride, self.padding)
        self.Z = self.Z + self.b

        if self.activation_fn is not None:
            self.A = self.activation_function(self.Z)
            return self.A
        return self.Z

    def backward(self, dA):
        if self.padding == 'valid':
            dA = dA
            dA = dA.reshape(dA.shape[0], dA.shape[1], -1)
            dZ = self.activation_function(self.Z, derivative=True)
            dAmdZ = dA * dZ.reshape(dZ.shape[0], dZ.shape[1], -1)
        elif self.padding == 'same':
            pseudo_dA = np.zeros(self.pseudoZ.shape)
            pseudo_dZ = np.zeros(self.pseudoZ.shape)
            pseudo_dA[:, :, self.row_start:self.row_end, self.col_start:self.col_end] = dA[:, :, :, :]
            dA = pseudo_dA
            dA = dA.reshape(dA.shape[0], dA.shape[1], -1)
            dZ = self.activation_function(self.Z, derivative=True)
            pseudo_dZ[:, :, self.row_start:self.row_end, self.col_start:self.col_end] = dZ[:, :, :, :]
            dZ = pseudo_dZ
            dAmdZ = dA * dZ.reshape(dZ.shape[0], dZ.shape[1], -1)

        df = np.zeros(self.f.shape)
        for ii in range(self.input.shape[0]):
            for ij in range(self.f.shape[0]):
                for ik in range(self.f.shape[1]):
                    for i in range(self.input.shape[2]):
                        for j in range(self.input.shape[3]):
                            fil_el_mat = np.zeros((self.f.shape[2], self.f.shape[3]))
                            for k in range(self.f.shape[2]):
                                for l in range(self.f.shape[3]):
                                    fil_el_mat[k, l] = \
                                        dAmdZ[ii, ij, i * self.stride[0] * dZ.shape[3] + \
                                        j * self.stride[1] + k * dZ.shape[3] + l]
                            df[ij, ik, :, :] += fil_el_mat * self.input[ii, ik, i, j]

        df = (1 / self.input.shape[0]) * df
        self.df = df

        db = (1 / self.input.shape[0]) * (np.sum(dAmdZ, axis=(0, 2))).reshape(self.b.shape)
        self.db = db

        dx = np.zeros(self.input.shape)
        for ii in range(self.input.shape[0]):
            for ij in range(self.f.shape[0]):
                for ik in range(self.f.shape[1]):
                    for i in range(self.input.shape[2]):
                        for j in range(self.input.shape[3]):
                            fil_el_mat2 = np.zeros((self.f.shape[2], self.f.shape[3]))
                            for k in range(self.f.shape[2]):
                                for l in range(self.f.shape[3]):
                                    fil_el_mat2[k, l] = \
                                        dAmdZ[ii, ij, i * self.stride[0] * dZ.shape[3] + \
                                        j * self.stride[1] + k * dZ.shape[3] + l]
                            dx[ii, ik, i, j] += np.sum(fil_el_mat2 * self.f[ij, ik, :, :])

        self.dx = dx
        return self.df, self.db, self.dx
